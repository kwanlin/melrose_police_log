---
title: "Melrose Police Log"
author: "Kwan Lin"
date: "7/19/2017"
output:
  hrbrthemes::ipsum:
    keep_md: true
editor_options:
  chunk_output_type: inline
---

# Setup

```{r setup, include=FALSE}
#setwd(dir = "~/projects/melrose_police_log/")
#knitr::opts_knit$set(root.dir = "~/pro jects/melrose_police_log/")
knitr::opts_chunk$set(message=FALSE,
                      warning=FALSE,
                      echo=FALSE,
                      fig.retina=2,
                      dev=c("png", "cairo_pdf"))
```

```{r packages}
library(tidyverse)
library(rprojroot)
library(rvest)
library(httr)
library(pbapply)
library(stringr)
# library(officer)
library(textreadr)

# themes
library(hrbrthemes)
```

```{r paths}
root_path <- find_rstudio_root_file()
```

# Overview

This is a project to extract and analyze data from Melrose police logs.

# References

https://stackoverflow.com/questions/33790052/download-all-files-from-a-folder-on-a-website



# Data

```{r scrape-melrose-police-logs}
log_url <- "https://melrosepolice.net/police-logs/"

url_contents <- read_html(log_url)

docs <- grep("docx", html_attr(html_nodes(url_contents, "a[href]"), "href"), value=TRUE)


# ignore errors
options(warn = -1)

# apply to a list of urls for .docx files - download each file to a path. includes error handling
lapply(docs, function(x) tryCatch(download.file(x,paste0(root_path,"/data/",basename(x))),
                                  error = function(e) print(paste(basename(x), 'did not work out'))))

# saveRDS(object, file = paste0(root_path,"/data/object.rds"))
```

```{r parse_function}
parse_log <- function(file_path) {
  file_contents <- textreadr::read_docx(file_path)
  
  file_df <- as.data.frame(file_contents)
  
  df_parse <- file_df %>%
    filter(grepl("\\d{2}-\\d{4,} \\d{4}|Location|Vicinity|Date", file_contents)) %>%
    mutate(date_tag = ifelse(grepl("Date",file_contents), 1, 0)) %>%
    mutate(datestamp = ifelse(grepl("Date",file_contents), stringr::str_extract(file_contents, "\\d+/\\d+/\\d+"),NA)) %>%
    fill(datestamp) %>%
    filter(!grepl("Date",file_contents)) %>%
    mutate(incident_tag = ifelse(grepl("\\d{2}-\\d{4,} \\d{4}", file_contents),1,0)) %>%
    mutate(inc_cnt = cumsum(incident_tag)) %>%
    spread(incident_tag, file_contents) %>%
    janitor::clean_names() %>%
    mutate(location_raw = gsub("Location/Address: |Vicinity of: ", "", x0)) %>%
    # mutate(location_split = strsplit(location_raw, " - ")) %>%
    mutate(location_name = stringr::str_extract(location_raw, "^(.+?) - "),
           location_name = gsub(" - ", "", location_name),
           location_name = trimws(location_name)) %>%
    mutate(location_addr = gsub("^(.+?) - ","", location_raw),
           location_addr = gsub("Apt.(.+?)$","", location_addr),
           location_addr = trimws(location_addr)) %>%
    mutate(incident_id = stringr::str_extract(x1, "\\d{2}-\\d{4,}")) %>% # \\d{4}
    mutate(hour_min = stringr::str_extract(x1, " \\d{4}")) %>%
    # mutate(accident = ifelse(grepl("ACCIDENT|accident", x1), 1, 0)) %>%
    select(-c(date_tag, inc_cnt))
}

```

```{r test-parse-func}
test_function <- parse_log("../data/Dispatch-Log-From-April-13-20-2015.docx")

test_function2 <- parse_log("../data/MPD-Dispatch-Log-Oct-22-to-Oct-28.docx")
```

```{r loop-over-all-files}
list_of_files <- list.files("../data/")

merged_logs <- do.call(rbind(lapply(list_of_files, parse_log)))
```

# Analysis